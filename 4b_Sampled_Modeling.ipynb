{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Predicting-if-the-Question-is-Answered\" data-toc-modified-id=\"Introduction:-Predicting-if-the-Question-is-Answered-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Predicting if the Question is Answered</a></span></li><li><span><a href=\"#Data-Frame-1\" data-toc-modified-id=\"Data-Frame-1-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Frame 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reading-in-DataFrame\" data-toc-modified-id=\"Reading-in-DataFrame-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Reading in DataFrame</a></span></li><li><span><a href=\"#Baseline-Accuracy\" data-toc-modified-id=\"Baseline-Accuracy-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Baseline Accuracy</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-Data-With-FunctionTransformer\" data-toc-modified-id=\"Transforming-Data-With-FunctionTransformer-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Transforming Data With FunctionTransformer</a></span></li><li><span><a href=\"#Sampling-the-Data-In-Order-to-Create-Even-Classes\" data-toc-modified-id=\"Sampling-the-Data-In-Order-to-Create-Even-Classes-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Sampling the Data In Order to Create Even Classes</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Instantiating-X-and-y-variables\" data-toc-modified-id=\"Instantiating-X-and-y-variables-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Instantiating X and y variables</a></span></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Train Test Split</a></span></li><li><span><a href=\"#Logistic-Regression-Gridsearch\" data-toc-modified-id=\"Logistic-Regression-Gridsearch-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Logistic Regression Gridsearch</a></span></li><li><span><a href=\"#KNN-Gridsearch\" data-toc-modified-id=\"KNN-Gridsearch-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>KNN Gridsearch</a></span></li><li><span><a href=\"#Random-Forest-Gridsearch\" data-toc-modified-id=\"Random-Forest-Gridsearch-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;</span>Random Forest Gridsearch</a></span></li></ul></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Confusion Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-Definition\" data-toc-modified-id=\"Function-Definition-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Function Definition</a></span></li><li><span><a href=\"#Function-Execution:-Confusion-Matrix-Results\" data-toc-modified-id=\"Function-Execution:-Confusion-Matrix-Results-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Function Execution: Confusion Matrix Results</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Frame-2\" data-toc-modified-id=\"Data-Frame-2-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Frame 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression-Gridsearch\" data-toc-modified-id=\"Logistic-Regression-Gridsearch-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Logistic Regression Gridsearch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Words-Most-Indicative-To-The-Question-Being-Answered.\" data-toc-modified-id=\"Words-Most-Indicative-To-The-Question-Being-Answered.-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Words Most Indicative To The Question Being Answered.</a></span></li></ul></li><li><span><a href=\"#Thank-You!!\" data-toc-modified-id=\"Thank-You!!-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Thank You!!</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Modeling\n",
    "\n",
    "## Introduction: Predicting if the Question is Answered\n",
    "\n",
    "To better understand the impact of how a question is asked, I built a model to see how well I could predict if a question was answered based on the question text and the question score. In this notebook, I grid-search over two sampled dataframes with several models.\n",
    "\n",
    "*Dataframe 1:*\n",
    "In the unsampled model from the notebook \"4a_Unsampled_Modeling.ipynb\", I performed a grid-search over the whole data frame using `questions_body` and `questions_score` as features. Since the classes were highly unbalanced, with a 98% baseline score, I decided to sample the data to even out the classes testing to see if I could predict if a question was answered. Both the `questions_body` and `questions_score` were kept as features. \n",
    "\n",
    "*Dataframe 2:*\n",
    "In the second dataframe, I used the same sampled dataframe as in \"dataframe 1\" but dropped the `questions_score` feature, since I hypothesized that `question_score` was a strong indicator of a question being answered. Additionally, it is easier to get real-time feedback on the text of your question, versus waiting for others to up-vote it, in order to increase your chances of the question being answered.\n",
    "\n",
    "For the first dataframe, I grid-search over Logistic Regression, K-Nearest Neighbors, and Random Forest, with Random Forest Providing the best results. \n",
    "\n",
    "In the second dataframe I only used Logicstic Regression due to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "    #Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "    #Sklearn Packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction import stop_words, text\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    #Suppress FutureWarning messages\\n\",\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame 1\n",
    "\n",
    "### Reading in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Datasets/cleaned_4_modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questions_id              0\n",
       "questions_author_id       0\n",
       "questions_date_added      0\n",
       "questions_title           0\n",
       "questions_body            0\n",
       "questions_score           8\n",
       "was_answered              0\n",
       "answers_score           837\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this classification model, we're not going to use the answers_score column and so I will drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='answers_score', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51944, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 8 rows with nulls out of about 52k, dropping the rows where there are nulls in the questions_score will not significantly affect the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy\n",
    "\n",
    "Since most of the questions are answered the classes are highly unbalanced with 98% Baseline Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.9841920825631546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    51115\n",
       "0.0      821\n",
       "Name: was_answered, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"baseline:\", df['was_answered'].mean())\n",
    "df['was_answered'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Data With FunctionTransformer\n",
    "\n",
    "In order to format the data for modeling I'm using a Function Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(lambda x: x['questions_body'], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['questions_score',]], validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the Data In Order to Create Even Classes\n",
    "\n",
    "Since the classes above are so unbalanced I'm only taking a sample of the data where the question was answered. This creates a new baseline accuracy near 50% so we can actually model and test how much impact our features have on being able to predict if the question is answered or not. If we did not balance the classes, the model could just predict a question as answered every time and it would be 98% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating portioned dataframes that have even classes of the dataframe where questions were and were not answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_was = df[df['was_answered']==1].sample(n=900)\n",
    "df_wasnt = df[df['was_answered']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the portioned dataframes above back into one dataframe to be used in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.concat([df_was, df_wasnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the new baseline accuracy score so we can compare how the models performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5229517722254503"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['was_answered'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample_df['was_answered']\n",
    "X = sample_df[['questions_body','questions_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Pipeline to Grid Search Using Standard Scaler, Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9410852713178295\n",
      "test score 0.8283062645011601\n",
      "best params: {'logreg__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "pipe_logreg = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "params_logreg = {\n",
    "           'logreg__penalty' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "gs_logreg = GridSearchCV(pipe_logreg, params_logreg, cv=5)\n",
    "\n",
    "gs_logreg.fit(X_train, y_train)\n",
    "print(\"train score\", gs_logreg.score(X_train, y_train))\n",
    "print(\"test score\", gs_logreg.score(X_test, y_test))\n",
    "print(\"best params:\", gs_logreg.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8434108527131783\n",
      "test score 0.6960556844547564\n",
      "best params: {'knn__n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "pipe_knn = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "\n",
    "])\n",
    "\n",
    "params_knn = {\n",
    "    'knn__n_neighbors' : [3, 5, 10, 15, 20],\n",
    "#     'knn__metric': ['euclidean', 'manhattan']  #Takes a while to run\n",
    "\n",
    "}\n",
    "\n",
    "gs_knn = GridSearchCV(pipe_knn, params_knn, cv=5)\n",
    "\n",
    "gs_knn.fit(X_train, y_train)\n",
    "print(\"train score:\", gs_knn.score(X_train, y_train))\n",
    "print(\"test score\", gs_knn.score(X_test, y_test))\n",
    "print(\"best params:\", gs_knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8790697674418605\n",
      "test score: 0.8491879350348028\n",
      "best params: {'rf__max_depth': 4, 'rf__max_features': None, 'rf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "\n",
    "])\n",
    "\n",
    "params_rf = {\n",
    "    'rf__n_estimators': [100,125],\n",
    "    'rf__max_depth': [None, 4, 5, 6],\n",
    "    'rf__max_features': [None,\"auto\"]}\n",
    "    \n",
    "\n",
    "gs_rf = GridSearchCV(pipe_rf, params_rf, cv=5)\n",
    "\n",
    "gs_rf.fit(X_train, y_train)\n",
    "print(\"train score:\", gs_rf.score(X_train, y_train))\n",
    "print(\"test score:\", gs_rf.score(X_test, y_test))\n",
    "print(\"best params:\", gs_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definition\n",
    "\n",
    "Defining a Function that Returns a Confusion Matrix as a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix provides evaluation metrics that highlight how the model is being accurate and erroneous. The confusion matrix below shows scores from the Random Forest Model which had the best prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion(y_test, preds, classes):\n",
    "\n",
    "    conmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy Score: {accuracy_score(y_test, preds)}')\n",
    "    print(f'Precision Score: {precision_score(y_test, preds)}')\n",
    "    print(f'Recall Score: {recall_score(y_test, preds)}')\n",
    "    return pd.DataFrame(conmat, columns=['Predicted ' +class_ for class_ in classes], \\\n",
    "                index=['Actual '+ class_ for class_ in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Execution: Confusion Matrix Results\n",
    "\n",
    "Calling `make_confusion` function to get Accuracy, Precision and Recalls Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8491879350348028\n",
      "Precision Score: 0.8458333333333333\n",
      "Recall Score: 0.8787878787878788\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted wasn't answered</th>\n",
       "      <th>Predicted was answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual wasn't answered</th>\n",
       "      <td>163</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual was answered</th>\n",
       "      <td>28</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Predicted wasn't answered  Predicted was answered\n",
       "Actual wasn't answered                        163                      37\n",
       "Actual was answered                            28                     203"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a function to print out a nice confusion matrix\n",
    "preds = gs_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "make_confusion(y_test, preds, [\"wasn't answered\", \"was answered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully modeling if a question will be answered or not based on question body and score, I wanted to know how we could predict just on question body since score was a likely tell. Below, I modeled using a pipeline and grid-searching with logistic regression. This data set still uses the sampled data and balanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the X and Y variable, with only `questions_body` used as a predictor for y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample_df['was_answered']\n",
    "X = sample_df['questions_body']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.72015504        nan 0.71937984        nan 0.70620155\n",
      "        nan 0.73023256        nan 0.71627907        nan 0.7248062\n",
      "        nan 0.70697674        nan 0.72635659        nan 0.72093023\n",
      "        nan 0.7248062         nan 0.70077519        nan 0.72403101\n",
      "        nan 0.72093023        nan 0.7248062         nan 0.70387597\n",
      "        nan 0.72403101]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9658914728682171\n",
      "test score: 0.7610208816705336\n",
      "Accuracy Score: 0.7610208816705336\n",
      "Precision Score: 0.7935779816513762\n",
      "Recall Score: 0.7489177489177489\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted wasn't answered</th>\n",
       "      <th>Predicted was answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual wasn't answered</th>\n",
       "      <td>155</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual was answered</th>\n",
       "      <td>58</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Predicted wasn't answered  Predicted was answered\n",
       "Actual wasn't answered                        155                      45\n",
       "Actual was answered                            58                     173"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logreg2 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(solver = 'liblinear'))\n",
    "])\n",
    "\n",
    "params_logreg2 = {\n",
    "    'cvec__stop_words' : [None, 'english'],\n",
    "    'logreg__penalty' : ['none','l2'],\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "\n",
    "gs_logreg2 = GridSearchCV(pipe_logreg2, # what object are we optimizing?\n",
    "                  params_logreg2, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation.\n",
    "\n",
    "gs_logreg2.fit(X_train2, y_train2)\n",
    "gs_logreg2_model = gs_logreg2.best_estimator_\n",
    "print(\"train score:\", gs_logreg2_model.score(X_train2, y_train2))\n",
    "print(\"test score:\", gs_logreg2_model.score(X_test2, y_test2))\n",
    "\n",
    "preds = gs_logreg2.best_estimator_.predict(X_test2)\n",
    "make_confusion(y_test, preds, [\"wasn't answered\", \"was answered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words Most Indicative To The Question Being Answered.\n",
    "Below I set up and output a dataframe with the coefficients (words most indicative to questions being answered or not answered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = gs_logreg2.best_estimator_.named_steps['logreg'].coef_[0]\n",
    "features = gs_logreg2.best_estimator_.named_steps['cvec'].get_feature_names_out()\n",
    "coef_df = pd.DataFrame({'features' : features,\n",
    "             'coefficients': coefs})\n",
    "coef_df.sort_values('coefficients', ascending = False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative to the question being answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>marketing</td>\n",
       "      <td>1.496103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>1.240649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>really</td>\n",
       "      <td>1.197965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>accounting</td>\n",
       "      <td>1.171201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>pay</td>\n",
       "      <td>1.123690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>career</td>\n",
       "      <td>1.118721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>love</td>\n",
       "      <td>1.094693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>social</td>\n",
       "      <td>1.086036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>tech</td>\n",
       "      <td>1.059557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>management</td>\n",
       "      <td>1.057625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>entrepreneurship</td>\n",
       "      <td>1.003460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>human</td>\n",
       "      <td>0.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>studying</td>\n",
       "      <td>0.975310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>plan</td>\n",
       "      <td>0.944922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>professional</td>\n",
       "      <td>0.938665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>engineers</td>\n",
       "      <td>0.927564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>art</td>\n",
       "      <td>0.911942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>scientist</td>\n",
       "      <td>0.904969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>film</td>\n",
       "      <td>0.883178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>finance</td>\n",
       "      <td>0.874487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  coefficients\n",
       "1131         marketing      1.496103\n",
       "1032            lawyer      1.240649\n",
       "1490            really      1.197965\n",
       "13          accounting      1.171201\n",
       "1331               pay      1.123690\n",
       "226             career      1.118721\n",
       "1089              love      1.094693\n",
       "1638            social      1.086036\n",
       "1761              tech      1.059557\n",
       "1124        management      1.057625\n",
       "588   entrepreneurship      1.003460\n",
       "833              human      0.985600\n",
       "1719          studying      0.975310\n",
       "1382              plan      0.944922\n",
       "1439      professional      0.938665\n",
       "576          engineers      0.927564\n",
       "112                art      0.911942\n",
       "1596         scientist      0.904969\n",
       "654               film      0.883178\n",
       "657            finance      0.874487"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values('coefficients', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative to the question NOT being answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>record</td>\n",
       "      <td>-0.777670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>certifications</td>\n",
       "      <td>-0.789060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>zoology</td>\n",
       "      <td>-0.814711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>accepted</td>\n",
       "      <td>-0.820964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>generation</td>\n",
       "      <td>-0.854646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>sure</td>\n",
       "      <td>-0.856681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>future career</td>\n",
       "      <td>-0.868673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>subject</td>\n",
       "      <td>-0.901526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>ultimately</td>\n",
       "      <td>-0.901788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>aerospace engineer</td>\n",
       "      <td>-0.963737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>pathology</td>\n",
       "      <td>-0.965481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>music business</td>\n",
       "      <td>-0.972617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>veterinarian</td>\n",
       "      <td>-0.995596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>aerospace</td>\n",
       "      <td>-1.021634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>loans</td>\n",
       "      <td>-1.030554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>medicine</td>\n",
       "      <td>-1.051444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>worth</td>\n",
       "      <td>-1.069525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>order</td>\n",
       "      <td>-1.075585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>programs</td>\n",
       "      <td>-1.076582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>healthcare hospital</td>\n",
       "      <td>-1.161483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 features  coefficients\n",
       "1507               record     -0.777670\n",
       "253        certifications     -0.789060\n",
       "1999              zoology     -0.814711\n",
       "9                accepted     -0.820964\n",
       "709            generation     -0.854646\n",
       "1735                 sure     -0.856681\n",
       "698         future career     -0.868673\n",
       "1724              subject     -0.901526\n",
       "1853           ultimately     -0.901788\n",
       "47     aerospace engineer     -0.963737\n",
       "1325            pathology     -0.965481\n",
       "1214       music business     -0.972617\n",
       "1880         veterinarian     -0.995596\n",
       "46              aerospace     -1.021634\n",
       "1069                loans     -1.030554\n",
       "1168             medicine     -1.051444\n",
       "1978                worth     -1.069525\n",
       "1297                order     -1.075585\n",
       "1447             programs     -1.076582\n",
       "783   healthcare hospital     -1.161483"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values('coefficients', ascending = False).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank You!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
