{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Predicting-if-the-Question-is-Answered\" data-toc-modified-id=\"Introduction:-Predicting-if-the-Question-is-Answered-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Predicting if the Question is Answered</a></span></li><li><span><a href=\"#Data-Frame-1\" data-toc-modified-id=\"Data-Frame-1-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Frame 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reading-in-DataFrame\" data-toc-modified-id=\"Reading-in-DataFrame-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Reading in DataFrame</a></span></li><li><span><a href=\"#Baseline-Accuracy\" data-toc-modified-id=\"Baseline-Accuracy-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Baseline Accuracy</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-Data-With-FunctionTransformer\" data-toc-modified-id=\"Transforming-Data-With-FunctionTransformer-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Transforming Data With FunctionTransformer</a></span></li><li><span><a href=\"#Sampling-the-Data-In-Order-to-Create-Even-Classes\" data-toc-modified-id=\"Sampling-the-Data-In-Order-to-Create-Even-Classes-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Sampling the Data In Order to Create Even Classes</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Instantiating-X-and-y-variables\" data-toc-modified-id=\"Instantiating-X-and-y-variables-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Instantiating X and y variables</a></span></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Train Test Split</a></span></li><li><span><a href=\"#Logistic-Regression-Gridsearch\" data-toc-modified-id=\"Logistic-Regression-Gridsearch-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Logistic Regression Gridsearch</a></span></li><li><span><a href=\"#KNN-Gridsearch\" data-toc-modified-id=\"KNN-Gridsearch-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>KNN Gridsearch</a></span></li><li><span><a href=\"#Random-Forest-Gridsearch\" data-toc-modified-id=\"Random-Forest-Gridsearch-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;</span>Random Forest Gridsearch</a></span></li></ul></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Confusion Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-Definition\" data-toc-modified-id=\"Function-Definition-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Function Definition</a></span></li><li><span><a href=\"#Function-Execution:-Confusion-Matrix-Results\" data-toc-modified-id=\"Function-Execution:-Confusion-Matrix-Results-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Function Execution: Confusion Matrix Results</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Frame-2\" data-toc-modified-id=\"Data-Frame-2-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Frame 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression-Gridsearch\" data-toc-modified-id=\"Logistic-Regression-Gridsearch-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Logistic Regression Gridsearch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Words-Most-Indicative-of-a-Question-Being-Answered.\" data-toc-modified-id=\"Words-Most-Indicative-of-a-Question-Being-Answered.-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Words Most Indicative of a Question Being Answered.</a></span></li></ul></li><li><span><a href=\"#Thank-You!!\" data-toc-modified-id=\"Thank-You!!-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Thank You!!</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Modeling\n",
    "\n",
    "## Introduction: Predicting if the Question is Answered\n",
    "\n",
    "To better understand the impact of how a question is asked, I built a model to see how well I could predict if a question was answered based on the question text and the question score. In this notebook, I grid-search over two sampled dataframes with several models.\n",
    "\n",
    "*Dataframe 1:*\n",
    "In the unsampled model from the notebook \"4a_Unsampled_Modeling.ipynb\", I performed a grid-search over the whole data frame using `questions_body` and `questions_score` as features. Since the classes were highly unbalanced, with a 98% baseline score, I decided to sample the data to even out the classes testing to see if I could predict if a question was answered. Both the `questions_body` and `questions_score` were kept as features. \n",
    "\n",
    "*Dataframe 2:*\n",
    "In the second dataframe, I used the same sampled dataframe as in \"dataframe 1\" but dropped the `questions_score` feature, since I hypothesized that `question_score` was a strong indicator of a question being answered. Additionally, it is easier to get real-time feedback on the text of your question, versus waiting for others to up-vote it, in order to increase your chances of the question being answered.\n",
    "\n",
    "For the first dataframe, I grid-search over Logistic Regression, K-Nearest Neighbors, and Random Forest, with Random Forest Providing the best results. \n",
    "\n",
    "In the second dataframe I only used Logicstic Regression due to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "    #Exporting python objects for later recall\n",
    "# using dill to export the GridSearchCV object\n",
    "# dill is an augmented version of pickle github.com/uqfoundation/dill\n",
    "# Since the GridSearchCV object contains a lambda function, regular pickle cannot dump or load it\n",
    "# also note that we are not using joblib here since we are not saving off large (>4GB) numpy arrays\n",
    "import dill as pickle\n",
    "\n",
    "\n",
    "    #Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "    #Sklearn Packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction import stop_words, text\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    #Suppress FutureWarning messages\\n\",\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame 1\n",
    "\n",
    "### Reading in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Datasets/cleaned_4_modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questions_id              0\n",
       "questions_author_id       0\n",
       "questions_date_added      0\n",
       "questions_title           0\n",
       "questions_body            0\n",
       "questions_score           8\n",
       "was_answered              0\n",
       "answers_score           837\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this classification model, we're not going to use the answers_score column and so I will drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='answers_score', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51944, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 8 rows with nulls out of about 52k, dropping the rows where there are nulls in the questions_score will not significantly affect the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy\n",
    "\n",
    "Since most of the questions are answered the classes are highly unbalanced with 98% Baseline Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.9841920825631546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    51115\n",
       "0.0      821\n",
       "Name: was_answered, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"baseline:\", df['was_answered'].mean())\n",
    "df['was_answered'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Data With FunctionTransformer\n",
    "\n",
    "In order to format the data for modeling I'm using a Function Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(lambda x: x['questions_body'], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['questions_score',]], validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the Data In Order to Create Even Classes\n",
    "\n",
    "Since the classes above are so unbalanced I'm only taking a sample of the data where the question was answered. This creates a new baseline accuracy near 50% so we can actually model and test how much impact our features have on being able to predict if the question is answered or not. If we did not balance the classes, the model could just predict a question as answered every time and it would be 98% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating portioned dataframes that have even classes of the dataframe where questions were and were not answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_was = df[df['was_answered']==1].sample(n=900)\n",
    "df_wasnt = df[df['was_answered']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the portioned dataframes above back into one dataframe to be used in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.concat([df_was, df_wasnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the new baseline accuracy score so we can compare how the models performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5229517722254503"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['was_answered'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample_df['was_answered']\n",
    "X = sample_df[['questions_body','questions_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the data to access later if it is desired to re-run the models using the same train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"./pickles/X_train_4b.pkl\")\n",
    "X_test.to_pickle(\"./pickles/X_test_4b.pkl\")\n",
    "y_train.to_pickle(\"./pickles/y_train_4b.pkl\")\n",
    "y_test.to_pickle(\"./pickles/y_test_4b.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded = pd.read_pickle(\"./pickles/X_train_4b.pkl\")\n",
    "X_test_loaded = pd.read_pickle(\"./pickles/X_test_4b.pkl\")\n",
    "y_train_loaded = pd.read_pickle(\"./pickles/y_train_4b.pkl\")\n",
    "y_test_loaded = pd.read_pickle(\"./pickles/y_test_4b.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Pipeline to Grid Search Using Standard Scaler, Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logreg = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "params_logreg = {\n",
    "           'logreg__penalty' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "gs_logreg = GridSearchCV(pipe_logreg, params_logreg, cv=5)\n",
    "gs_logreg.fit(X_train, y_train)\n",
    "\n",
    "with open(\"./pickles/gs_logreg_4b.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_logreg, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.951937984496124\n",
      "test score 0.8190255220417634\n",
      "best params: {'logreg__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_logreg_4b.pkl\", 'rb') as f:\n",
    "    gs_logreg_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_logreg_loaded.score(X_train_loaded, y_train_loaded))\n",
    "print(\"test score\", gs_logreg_loaded.score(X_test_loaded, y_test_loaded))\n",
    "print(\"best params:\", gs_logreg_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_knn = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "\n",
    "])\n",
    "\n",
    "params_knn = {\n",
    "    'knn__n_neighbors' : [3, 5, 10, 15, 20],\n",
    "#     'knn__metric': ['euclidean', 'manhattan']  #Takes a while to run\n",
    "\n",
    "}\n",
    "\n",
    "gs_knn = GridSearchCV(pipe_knn, params_knn, cv=5)\n",
    "gs_knn.fit(X_train, y_train)\n",
    "\n",
    "with open(\"./pickles/gs_knn_4b.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_knn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.8372093023255814\n",
      "test score 0.6705336426914154\n",
      "best params: {'knn__n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_knn_4b.pkl\", 'rb') as f:\n",
    "    gs_knn_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_knn_loaded.score(X_train_loaded, y_train_loaded))\n",
    "print(\"test score\", gs_knn_loaded.score(X_test_loaded, y_test_loaded))\n",
    "print(\"best params:\", gs_knn_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "\n",
    "])\n",
    "\n",
    "params_rf = {\n",
    "    'rf__n_estimators': [100,125],\n",
    "    'rf__max_depth': [None, 4, 5, 6],\n",
    "    'rf__max_features': [None,\"auto\"]}\n",
    "    \n",
    "\n",
    "gs_rf = GridSearchCV(pipe_rf, params_rf, cv=5)\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "with open(\"./pickles/gs_rf_4b.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.8821705426356589\n",
      "test score 0.8329466357308585\n",
      "best params: {'rf__max_depth': 4, 'rf__max_features': None, 'rf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_rf_4b.pkl\", 'rb') as f:\n",
    "    gs_rf_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_rf_loaded.score(X_train_loaded, y_train_loaded))\n",
    "print(\"test score\", gs_rf_loaded.score(X_test_loaded, y_test_loaded))\n",
    "print(\"best params:\", gs_rf_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definition\n",
    "\n",
    "Defining a Function that Returns a Confusion Matrix as a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix provides evaluation metrics that highlight how the model is being accurate and erroneous. The confusion matrix below shows scores from the Random Forest Model which had the best prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion(y_test, preds, classes):\n",
    "\n",
    "    conmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy Score: {accuracy_score(y_test, preds)}')\n",
    "    print(f'Precision Score: {precision_score(y_test, preds)}')\n",
    "    print(f'Recall Score: {recall_score(y_test, preds)}')\n",
    "    return pd.DataFrame(conmat, columns=['Predicted ' +class_ for class_ in classes], \\\n",
    "                index=['Actual '+ class_ for class_ in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Execution: Confusion Matrix Results\n",
    "\n",
    "Calling `make_confusion` function to get Accuracy, Precision and Recalls Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8329466357308585\n",
      "Precision Score: 0.8502202643171806\n",
      "Recall Score: 0.8354978354978355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted wasn't answered</th>\n",
       "      <th>Predicted was answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual wasn't answered</th>\n",
       "      <td>166</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual was answered</th>\n",
       "      <td>38</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Predicted wasn't answered  Predicted was answered\n",
       "Actual wasn't answered                        166                      34\n",
       "Actual was answered                            38                     193"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a function to print out a nice confusion matrix\n",
    "preds = gs_rf_loaded.best_estimator_.predict(X_test)\n",
    "\n",
    "make_confusion(y_test, preds, [\"wasn't answered\", \"was answered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully modeling if a question will be answered or not based on question body and score, I wanted to know how we could predict just on question body since score was a likely tell. Below, I modeled using a pipeline and grid-searching with logistic regression. This data set still uses the sampled data and balanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the X and Y variable, with only `questions_body` used as a predictor for y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample_df['was_answered']\n",
    "X = sample_df['questions_body']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the data to access later if it is desired to re-run the models using the same train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2.to_pickle(\"./pickles/X_train2_4b.pkl\")\n",
    "X_test2.to_pickle(\"./pickles/X_test2_4b.pkl\")\n",
    "y_train2.to_pickle(\"./pickles/y_train2_4b.pkl\")\n",
    "y_test2.to_pickle(\"./pickles/y_test2_4b.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_loaded = pd.read_pickle(\"./pickles/X_train2_4b.pkl\")\n",
    "X_test2_loaded = pd.read_pickle(\"./pickles/X_test2_4b.pkl\")\n",
    "y_train2_loaded = pd.read_pickle(\"./pickles/y_train2_4b.pkl\")\n",
    "y_test2_loaded = pd.read_pickle(\"./pickles/y_test2_4b.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.72403101        nan 0.74108527        nan 0.71472868\n",
      "        nan 0.7503876         nan 0.72325581        nan 0.7503876\n",
      "        nan 0.71937984        nan 0.74573643        nan 0.72868217\n",
      "        nan 0.7496124         nan 0.72635659        nan 0.74728682\n",
      "        nan 0.72868217        nan 0.7496124         nan 0.72945736\n",
      "        nan 0.7496124 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe_logreg2 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(solver = 'liblinear'))\n",
    "])\n",
    "\n",
    "params_logreg2 = {\n",
    "    'cvec__stop_words' : [None, 'english'],\n",
    "    'logreg__penalty' : ['none','l2'],\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "\n",
    "gs_logreg2 = GridSearchCV(pipe_logreg2, # what object are we optimizing?\n",
    "                  params_logreg2, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation.\n",
    "gs_logreg2.fit(X_train2, y_train2)\n",
    "\n",
    "with open(\"./pickles/gs_logreg2_4b.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_logreg2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9643410852713178\n",
      "test score: 0.7540603248259861\n",
      "Accuracy Score: 0.7540603248259861\n",
      "Precision Score: 0.7828054298642534\n",
      "Recall Score: 0.7489177489177489\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted wasn't answered</th>\n",
       "      <th>Predicted was answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual wasn't answered</th>\n",
       "      <td>152</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual was answered</th>\n",
       "      <td>58</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Predicted wasn't answered  Predicted was answered\n",
       "Actual wasn't answered                        152                      48\n",
       "Actual was answered                            58                     173"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_logreg2_4b.pkl\", 'rb') as f:\n",
    "    gs_logreg2_loaded = pickle.load(f)\n",
    "\n",
    "gs_logreg2_model = gs_logreg2_loaded.best_estimator_\n",
    "print(\"train score:\", gs_logreg2_model.score(X_train2, y_train2))\n",
    "print(\"test score:\", gs_logreg2_model.score(X_test2, y_test2))\n",
    "\n",
    "preds = gs_logreg2_loaded.best_estimator_.predict(X_test2)\n",
    "make_confusion(y_test, preds, [\"wasn't answered\", \"was answered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words Most Indicative of a Question Being Answered.\n",
    "Below I set up and output a dataframe with the coefficients (words most indicative to questions being answered or not answered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = gs_logreg2_loaded.best_estimator_.named_steps['logreg'].coef_[0]\n",
    "features = gs_logreg2_loaded.best_estimator_.named_steps['cvec'].get_feature_names_out()\n",
    "coef_df = pd.DataFrame({'features' : features,\n",
    "             'coefficients': coefs})\n",
    "coef_df.sort_values('coefficients', ascending = False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative to the question being answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>marketing</td>\n",
       "      <td>1.319628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>accounting</td>\n",
       "      <td>1.244164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>love</td>\n",
       "      <td>1.193435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>tech</td>\n",
       "      <td>1.133778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>management</td>\n",
       "      <td>1.133040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>development</td>\n",
       "      <td>1.093669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>1.026275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>career</td>\n",
       "      <td>1.026116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>relations</td>\n",
       "      <td>1.002585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>soccer</td>\n",
       "      <td>0.990868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>human</td>\n",
       "      <td>0.974835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>computer</td>\n",
       "      <td>0.954235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>programming</td>\n",
       "      <td>0.947187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>trying</td>\n",
       "      <td>0.924663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>stress</td>\n",
       "      <td>0.903327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>sciences</td>\n",
       "      <td>0.895387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>sort</td>\n",
       "      <td>0.887512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>internship</td>\n",
       "      <td>0.885133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>pediatric nursing</td>\n",
       "      <td>0.870691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>technology</td>\n",
       "      <td>0.854188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features  coefficients\n",
       "1119          marketing      1.319628\n",
       "12           accounting      1.244164\n",
       "1079               love      1.193435\n",
       "1765               tech      1.133778\n",
       "1113         management      1.133040\n",
       "485         development      1.093669\n",
       "1023             lawyer      1.026275\n",
       "224              career      1.026116\n",
       "1498          relations      1.002585\n",
       "1634             soccer      0.990868\n",
       "828               human      0.974835\n",
       "375            computer      0.954235\n",
       "1417        programming      0.947187\n",
       "1842             trying      0.924663\n",
       "1703             stress      0.903327\n",
       "1585           sciences      0.895387\n",
       "1652               sort      0.887512\n",
       "915          internship      0.885133\n",
       "1314  pediatric nursing      0.870691\n",
       "1769         technology      0.854188"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values('coefficients', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative to the question NOT being answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>dance</td>\n",
       "      <td>-0.741815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>applied</td>\n",
       "      <td>-0.750163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>mechanic</td>\n",
       "      <td>-0.756761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>certification</td>\n",
       "      <td>-0.758330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>live</td>\n",
       "      <td>-0.764204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>space</td>\n",
       "      <td>-0.783693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>double</td>\n",
       "      <td>-0.787369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>gynecology</td>\n",
       "      <td>-0.801541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>dental</td>\n",
       "      <td>-0.812093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>future career</td>\n",
       "      <td>-0.818626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>ve</td>\n",
       "      <td>-0.838441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>scholarships</td>\n",
       "      <td>-0.864614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>worth</td>\n",
       "      <td>-0.880872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>exam</td>\n",
       "      <td>-0.882472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>willing</td>\n",
       "      <td>-0.882960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>im interested</td>\n",
       "      <td>-0.899498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>production music</td>\n",
       "      <td>-0.934869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>healthcare hospital</td>\n",
       "      <td>-1.008451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>pathology</td>\n",
       "      <td>-1.080656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>veterinarian</td>\n",
       "      <td>-1.097376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 features  coefficients\n",
       "447                 dance     -0.741815\n",
       "85                applied     -0.750163\n",
       "1137             mechanic     -0.756761\n",
       "256         certification     -0.758330\n",
       "1056                 live     -0.764204\n",
       "1655                space     -0.783693\n",
       "523                double     -0.787369\n",
       "761            gynecology     -0.801541\n",
       "471                dental     -0.812093\n",
       "699         future career     -0.818626\n",
       "1882                   ve     -0.838441\n",
       "1540         scholarships     -0.864614\n",
       "1980                worth     -0.880872\n",
       "614                  exam     -0.882472\n",
       "1948              willing     -0.882960\n",
       "855         im interested     -0.899498\n",
       "1407     production music     -0.934869\n",
       "776   healthcare hospital     -1.008451\n",
       "1303            pathology     -1.080656\n",
       "1885         veterinarian     -1.097376"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values('coefficients', ascending = False).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank You!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 482.482888,
   "position": {
    "height": "40px",
    "left": "945.106px",
    "right": "20px",
    "top": "122.99px",
    "width": "578.655px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
