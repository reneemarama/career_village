{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Predicting If the Question Was Answered\n",
    "\n",
    "To better understand the impact of how the question was asked, I built a model to see how well I could predict if the question was answered. In this notebook, I grid-search over two sampled dataframes with several models.\n",
    "\n",
    "In the unsampled model from the previous notebook, I grid-search over the whole data frame using `questions_body` and `questions_score` as features. Since the classes were highly unbalanced, 98% baseline score, I decided to sample the data to even out the classes testing to see if I can predict if a question was answered. In the second dataframe, there was the sampled data set, with only `questions_body` as a feature, since I hypothesized that `question_score` was a strong indicator if the question was answered. \n",
    "\n",
    "For the first dataframe, I grid-search over Logistic Regression, K-Nearest Neighbors, and Random Forest, with Random Forest Providing the Best Results. \n",
    "\n",
    "In the second dataframe I only used Logicstic Regression due to time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model, I grid-searched over Logistic Regression and began a K-Nearest Neighbors but after about 8 hrs I terminated the kernel - especially because Logistic Regression was providing 99% test accuracy (baseline of 98%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "    #Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "    #Sklearn Packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction import stop_words, text\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Datasets/cleaned_4_modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questions_id              0\n",
       "questions_author_id       0\n",
       "questions_date_added      0\n",
       "questions_title           0\n",
       "questions_body            0\n",
       "questions_score           8\n",
       "was_answered              0\n",
       "answers_score           837\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this classification model, we're not going to use the answers_score column and so I will drop it. Since there are only 8 rows will nulls out of about 52k, dropping the rows where there are nulls in the questions_score shouldn't affect the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='answers_score', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping rows where there are nulls in question score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy\n",
    "\n",
    "Since most of the questions are answered the classes are highly unbalanced with 98% Baseline Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.9841920825631546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    51115\n",
       "0.0      821\n",
       "Name: was_answered, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"baseline:\", df['was_answered'].mean())\n",
    "df['was_answered'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Data With FunctionTransformer\n",
    "\n",
    "In order to format the data for modeling I'm using a Function Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(lambda x: x['questions_body'], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['questions_score',]], validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Data In Order to Create Even Classes\n",
    "\n",
    "Since the classes above are so unbalanced I'm only taking a sample of the data were the question was answered. This creates a new baseline accuracy of 53% so we can actually model and test how much impact our features have on being able to predict if the question is answered or not (if we balance the classes the model could just predict was answered every time and would be 98% correct).\n",
    "\n",
    "Creating portioned dataframes that have even classes of the dataframe where questions were and were not answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_was = df[df['was_answered']==1].head(900)\n",
    "df_wasnt = df[df['was_answered']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the portioned dataframes above back into one dataframe to be used in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.concat([df_was, df_wasnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the new baseline accuracy score so we can compare how the models performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5229517722254503"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['was_answered'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample_df['was_answered']\n",
    "X = sample_df[['questions_body','questions_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logicstic Regression Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Pipeline to Grid Search Using Standard Scaler, Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9418604651162791\n",
      "test score 0.8097447795823666\n",
      "best params: {'logreg__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "pipe_logreg = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "params = {\n",
    "           'logreg__penalty' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_logreg, params, cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"train score\", gs.score(X_train, y_train))\n",
    "print(\"test score\", gs.score(X_test, y_test))\n",
    "print(\"best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8992248062015504\n",
      "test score 0.7587006960556845\n",
      "best params: {'knn__n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "pipe_knn = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'knn__n_neighbors' : [3, 5, 10, 15, 20],\n",
    "#     'knn__metric': ['euclidean', 'manhattan']  #Takes a while to run\n",
    "\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_knn, params, cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"train score:\", gs.score(X_train, y_train))\n",
    "print(\"test score\", gs.score(X_test, y_test))\n",
    "print(\"best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.8793503480278422\n",
      "best params: {'rf__max_depth': None, 'rf__max_features': 'auto', 'rf__n_estimators': 125}\n"
     ]
    }
   ],
   "source": [
    "pipe_knn = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'rf__n_estimators': [100,125],\n",
    "    'rf__max_depth': [None, 4, 5, 6],\n",
    "    'rf__max_features': [None,\"auto\"]}\n",
    "    \n",
    "\n",
    "\n",
    "gs = GridSearchCV(pipe_knn, params, cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"train score:\", gs.score(X_train, y_train))\n",
    "print(\"test score:\", gs.score(X_test, y_test))\n",
    "print(\"best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a Function that Returns a Confusion Matrix as a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix provides evaluation metrics that highlight how the model is being accurate and erroneous. The confusion matrix below shows scores from the Random Forest Model which had the best prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion(y_test, preds, classes):\n",
    "\n",
    "    conmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy Score: {accuracy_score(y_test, preds)}')\n",
    "    print(f'Precision Score: {precision_score(y_test, preds)}')\n",
    "    print(f'Recall Score: {recall_score(y_test, preds)}')\n",
    "    return pd.DataFrame(conmat, columns=['Predicted ' +class_ for class_ in classes], \\\n",
    "                index=['Actual '+ class_ for class_ in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling `make_confusion` function to get Accuracy, Precision and Recalls Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8793503480278422\n",
      "Precision Score: 0.9013452914798207\n",
      "Recall Score: 0.8701298701298701\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted wasn't answered</th>\n",
       "      <th>Predicted was answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual wasn't answered</th>\n",
       "      <td>178</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual was answered</th>\n",
       "      <td>30</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Predicted wasn't answered  Predicted was answered\n",
       "Actual wasn't answered                        178                      22\n",
       "Actual was answered                            30                     201"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a function to print out a nice confusion matrix\n",
    "preds = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "make_confusion(y_test, preds, [\"wasn't answered\", \"was answered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully modeling if a question will be answered or not based on question body and score, I wanted to know how we could predict just on question body since score was a likely tell. Below I modeled using a pipeline and grid-searching with logistic regression. Keep in mind, the below is with the sampled data and balanced classes.\n",
    "\n",
    "Setting up the X and Y variable, with only `questions_body` used as a predictor for y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample_df['was_answered']\n",
    "X = sample_df['questions_body']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logicstic Regression Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rwilkening/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/rwilkening/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9674418604651163\n",
      "test score: 0.8283062645011601\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(solver = 'lbfgs'))\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cvec__stop_words' : [None, 'english'],\n",
    "    'logreg__penalty' : ['none','l2'],\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation.\n",
    "gs.fit(X_train, y_train)\n",
    "preds = gs.best_estimator_.predict(X_test)\n",
    "gs_model = gs.best_estimator_\n",
    "print(\"train score:\", gs_model.score(X_train, y_train))\n",
    "print(\"test score:\", gs_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rwilkening/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/rwilkening/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        tok...\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'cvec__stop_words': [None, 'english'],\n",
       "                         'logreg__penalty': ['none', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)\n",
    "preds = gs.best_estimator_.predict(X_test)\n",
    "gs_model = gs.best_estimator_\n",
    "print(\"train score:\", gs_model.score(X_train, y_train))\n",
    "print(\"test score:\", gs_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9674418604651163\n",
      "test score: 0.8283062645011601\n"
     ]
    }
   ],
   "source": [
    "preds = gs.best_estimator_.predict(X_test)\n",
    "gs_model = gs.best_estimator_\n",
    "print(\"train score:\", gs_model.score(X_train, y_train))\n",
    "print(\"test score:\", gs_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words Most Indicative To The Question Being Answered.\n",
    "Below I set up and output a dataframe with the coefficients (words most indicative to questions being answered or not answered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>csusmfreshman</td>\n",
       "      <td>1.916582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>entrepreneurship</td>\n",
       "      <td>1.404943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>hard</td>\n",
       "      <td>1.318574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>accounting</td>\n",
       "      <td>1.316048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>1.252850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>physical</td>\n",
       "      <td>-0.945298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>admissions</td>\n",
       "      <td>-1.022460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>aerospace</td>\n",
       "      <td>-1.123089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>tips</td>\n",
       "      <td>-1.138122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>considering</td>\n",
       "      <td>-1.203624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  coefficients\n",
       "656      csusmfreshman      1.916582\n",
       "952   entrepreneurship      1.404943\n",
       "1291              hard      1.318574\n",
       "19          accounting      1.316048\n",
       "1619            lawyer      1.252850\n",
       "...                ...           ...\n",
       "2002          physical     -0.945298\n",
       "48          admissions     -1.022460\n",
       "73           aerospace     -1.123089\n",
       "2720              tips     -1.138122\n",
       "584        considering     -1.203624\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = gs.best_estimator_.named_steps['logreg'].coef_[0]\n",
    "features = gs.best_estimator_.named_steps['cvec'].get_feature_names()\n",
    "coef_df = pd.DataFrame({'features' : features,\n",
    "             'coefficients': coefs})\n",
    "coef_df.sort_values('coefficients', ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative to the question being answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>csusmfreshman</td>\n",
       "      <td>1.916582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>entrepreneurship</td>\n",
       "      <td>1.404943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>hard</td>\n",
       "      <td>1.318574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>accounting</td>\n",
       "      <td>1.316048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>1.252850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>social</td>\n",
       "      <td>1.214599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>military</td>\n",
       "      <td>1.176548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>interviews</td>\n",
       "      <td>1.148334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>computer</td>\n",
       "      <td>1.103817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>media</td>\n",
       "      <td>1.083464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>love</td>\n",
       "      <td>1.080329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>carpentry</td>\n",
       "      <td>1.060389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>doubt</td>\n",
       "      <td>1.060389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>programming</td>\n",
       "      <td>1.028707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>industry</td>\n",
       "      <td>1.015752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>career</td>\n",
       "      <td>1.013876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>bioengineering</td>\n",
       "      <td>0.987245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>assign</td>\n",
       "      <td>0.958909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>figures</td>\n",
       "      <td>0.921977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>research</td>\n",
       "      <td>0.921821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  coefficients\n",
       "656      csusmfreshman      1.916582\n",
       "952   entrepreneurship      1.404943\n",
       "1291              hard      1.318574\n",
       "19          accounting      1.316048\n",
       "1619            lawyer      1.252850\n",
       "2460            social      1.214599\n",
       "1756          military      1.176548\n",
       "1521        interviews      1.148334\n",
       "566           computer      1.103817\n",
       "1730             media      1.083464\n",
       "1677              love      1.080329\n",
       "415          carpentry      1.060389\n",
       "832              doubt      1.060389\n",
       "2107       programming      1.028707\n",
       "1467          industry      1.015752\n",
       "412             career      1.013876\n",
       "335     bioengineering      0.987245\n",
       "231             assign      0.958909\n",
       "1106           figures      0.921977\n",
       "2244          research      0.921821"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values('coefficients', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative to the question NOT being answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>criminaljustice</td>\n",
       "      <td>-0.757701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>premed</td>\n",
       "      <td>-0.765540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>pathology</td>\n",
       "      <td>-0.780744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>choice</td>\n",
       "      <td>-0.793528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>minor</td>\n",
       "      <td>-0.802987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>computers</td>\n",
       "      <td>-0.806896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>paying</td>\n",
       "      <td>-0.830597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>help</td>\n",
       "      <td>-0.833407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>biochemistry</td>\n",
       "      <td>-0.867590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>colleges</td>\n",
       "      <td>-0.898431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>exam</td>\n",
       "      <td>-0.901192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>veterinarian</td>\n",
       "      <td>-0.901550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>difficult</td>\n",
       "      <td>-0.916432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>gap</td>\n",
       "      <td>-0.920258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>arts</td>\n",
       "      <td>-0.939941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>physical</td>\n",
       "      <td>-0.945298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>admissions</td>\n",
       "      <td>-1.022460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>aerospace</td>\n",
       "      <td>-1.123089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>tips</td>\n",
       "      <td>-1.138122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>considering</td>\n",
       "      <td>-1.203624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             features  coefficients\n",
       "643   criminaljustice     -0.757701\n",
       "2068           premed     -0.765540\n",
       "1951        pathology     -0.780744\n",
       "455            choice     -0.793528\n",
       "1759            minor     -0.802987\n",
       "567         computers     -0.806896\n",
       "1959           paying     -0.830597\n",
       "1323             help     -0.833407\n",
       "334      biochemistry     -0.867590\n",
       "512          colleges     -0.898431\n",
       "995              exam     -0.901192\n",
       "2862     veterinarian     -0.901550\n",
       "776         difficult     -0.916432\n",
       "1199              gap     -0.920258\n",
       "215              arts     -0.939941\n",
       "2002         physical     -0.945298\n",
       "48         admissions     -1.022460\n",
       "73          aerospace     -1.123089\n",
       "2720             tips     -1.138122\n",
       "584       considering     -1.203624"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values('coefficients', ascending = False).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank You!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
