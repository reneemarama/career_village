{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Reading-in-DataFrame\" data-toc-modified-id=\"Reading-in-DataFrame-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reading in DataFrame</a></span><ul class=\"toc-item\"><li><span><a href=\"#Removing-'answer'-columns-from-dataframe.\" data-toc-modified-id=\"Removing-'answer'-columns-from-dataframe.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Removing 'answer' columns from dataframe.</a></span></li></ul></li><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Baseline</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-Data-With-FunctionTransformer\" data-toc-modified-id=\"Transforming-Data-With-FunctionTransformer-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Transforming Data With FunctionTransformer</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Instantiating-our-X-and-y-variables\" data-toc-modified-id=\"Instantiating-our-X-and-y-variables-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Instantiating our X and y variables</a></span></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Train Test Split</a></span></li></ul></li><li><span><a href=\"#Logicstic-Regression\" data-toc-modified-id=\"Logicstic-Regression-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Logicstic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-a-Pipeline-to-Grid-Search-Using-Standard-Scaler,-Count-Vectorizer\" data-toc-modified-id=\"Building-a-Pipeline-to-Grid-Search-Using-Standard-Scaler,-Count-Vectorizer-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Building a Pipeline to Grid Search Using Standard Scaler, Count Vectorizer</a></span></li></ul></li><li><span><a href=\"#KNN\" data-toc-modified-id=\"KNN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>KNN</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-a-Function-that-Returns-a-Confusion-Matrix-as-a-DataFrame\" data-toc-modified-id=\"Setting-up-a-Function-that-Returns-a-Confusion-Matrix-as-a-DataFrame-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Setting up a Function that Returns a Confusion Matrix as a DataFrame</a></span></li><li><span><a href=\"#Calling-make_confusion-function-to-get-Accuracy,-Precision-and-Recalls-Scores-and-Confusion-Matrix\" data-toc-modified-id=\"Calling-make_confusion-function-to-get-Accuracy,-Precision-and-Recalls-Scores-and-Confusion-Matrix-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Calling <code>make_confusion</code> function to get Accuracy, Precision and Recalls Scores and Confusion Matrix</a></span></li></ul></li><li><span><a href=\"#Coefficients---Words-Most-Indicitive-if-the-Question-Will-Get-Answered-or-Not\" data-toc-modified-id=\"Coefficients---Words-Most-Indicitive-if-the-Question-Will-Get-Answered-or-Not-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Coefficients - Words Most Indicitive if the Question Will Get Answered or Not</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-Logistic-Regression-For-Extracting-Coefficients\" data-toc-modified-id=\"Setting-up-Logistic-Regression-For-Extracting-Coefficients-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Setting up Logistic Regression For Extracting Coefficients</a></span></li><li><span><a href=\"#Extracting-Features-and-Coefficients\" data-toc-modified-id=\"Extracting-Features-and-Coefficients-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Extracting Features and Coefficients</a></span></li><li><span><a href=\"#Creating-A-Data-Frame-with-Coefficients-and-Exponentiated-Coefficients\" data-toc-modified-id=\"Creating-A-Data-Frame-with-Coefficients-and-Exponentiated-Coefficients-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Creating A Data Frame with Coefficients and Exponentiated Coefficients</a></span></li><li><span><a href=\"#Words-Most-Indicitive-of-a-Question-Being-Answered\" data-toc-modified-id=\"Words-Most-Indicitive-of-a-Question-Being-Answered-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Words Most Indicitive of a Question Being Answered</a></span></li><li><span><a href=\"#Words-Most-Indicitive-of-a-Question-NOT-Being-Answered\" data-toc-modified-id=\"Words-Most-Indicitive-of-a-Question-NOT-Being-Answered-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>Words Most Indicitive of a Question NOT Being Answered</a></span></li><li><span><a href=\"#Graphing-Top-Words-for-Questions-Answered\" data-toc-modified-id=\"Graphing-Top-Words-for-Questions-Answered-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>Graphing Top Words for Questions Answered</a></span></li><li><span><a href=\"#Graphing-Top-Words-for-Questions-NOT-Answered\" data-toc-modified-id=\"Graphing-Top-Words-for-Questions-NOT-Answered-9.7\"><span class=\"toc-item-num\">9.7&nbsp;&nbsp;</span>Graphing Top Words for Questions NOT Answered</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsampled Raw Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "    #Exporting python objects for later recall\n",
    "# using dill to export the GridSearchCV object\n",
    "# dill is an augmented version of pickle github.com/uqfoundation/dill\n",
    "# Since the GridSearchCV object contains a lambda function, regular pickle cannot dump or load it\n",
    "# also note that we are not using joblib here since we are not saving off large (>4GB) numpy arrays\n",
    "import dill as pickle\n",
    "\n",
    "    #Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "    #Sklearn Packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from sklearn.feature_extraction import stop_words, text\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "    #Suppress FutureWarning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Datasets/cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questions_id               0\n",
       "questions_author_id        0\n",
       "questions_date_added       0\n",
       "questions_title            0\n",
       "questions_body             0\n",
       "questions_score            0\n",
       "tag_id                     0\n",
       "tag_name                   0\n",
       "answers_id                 0\n",
       "answers_author_id          0\n",
       "answers_date_added      2340\n",
       "answers_body              24\n",
       "answers_score              0\n",
       "was_answered               0\n",
       "has_tag                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 'answer' columns from dataframe.\n",
    "\n",
    "Since we don't want any bleeding in our prediction data, we're removing all the 'answer' columns. This also get's rid of all our null values. We're keeping `was_answered` because that will be our y-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions_id            0\n",
      "questions_author_id     0\n",
      "questions_date_added    0\n",
      "questions_title         0\n",
      "questions_body          0\n",
      "questions_score         0\n",
      "tag_id                  0\n",
      "tag_name                0\n",
      "was_answered            0\n",
      "has_tag                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Using a for loop to select all our columns with answer/answers in the name\n",
    "answer_cols = []\n",
    "for cols in df.columns:\n",
    "    if 'answers' in cols:\n",
    "        answer_cols.append(cols)\n",
    "        \n",
    "#Dropping answer cols\n",
    "df.drop(columns = answer_cols, inplace= True)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    178036\n",
       "0      2340\n",
       "Name: was_answered, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['was_answered'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data With FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(lambda x: x['questions_body'], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['questions_score', 'tag_id']], validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating our X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['was_answered']\n",
    "X = df[['questions_body','questions_score', 'tag_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"./pickles/X_train_4a.pkl\")\n",
    "X_test.to_pickle(\"./pickles/X_test_4a.pkl\")\n",
    "y_train.to_pickle(\"./pickles/y_train_4a.pkl\")\n",
    "y_test.to_pickle(\"./pickles/y_test_4a.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded = pd.read_pickle(\"./pickles/X_train_4a.pkl\")\n",
    "X_test_loaded = pd.read_pickle(\"./pickles/X_test_4a.pkl\")\n",
    "y_train_loaded = pd.read_pickle(\"./pickles/y_train_4a.pkl\")\n",
    "y_test_loaded = pd.read_pickle(\"./pickles/y_test_4a.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logicstic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Pipeline to Grid Search Using Standard Scaler, Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logreg = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "params = {\n",
    "           'logreg__penalty' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "gs_logreg = GridSearchCV(pipe_logreg, params, cv=5)\n",
    "gs_logreg.fit(X_train, y_train)\n",
    "\n",
    "with open(\"./pickles/gs_logreg_4a.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_logreg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.996281840895315\n",
      "test score 0.9948773672772431\n",
      "best params: {'logreg__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_logreg_4a.pkl\", 'rb') as f:\n",
    "    gs_logreg_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_logreg_loaded.score(X_train, y_train))\n",
    "print(\"test score\", gs_logreg_loaded.score(X_test, y_test))\n",
    "print(\"best params:\", gs_logreg_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe_knn = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'knn__n_neighbors' : [3, 5, 10, 15],\n",
    "#     'knn__metric': ['euclidean', 'manhattan']  #Takes a while to run\n",
    "\n",
    "}\n",
    "\n",
    "gs_knn = GridSearchCV(pipe_knn, params, cv=5)\n",
    "gs_knn.fit(X_train, y_train)\n",
    "\n",
    "with open(\"./pickles/gs_knn_4a.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_knn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9976345707485105\n",
      "test score 0.9936798687186765\n",
      "best params: {'knn__n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_knn_4a.pkl\", 'rb') as f:\n",
    "    gs_knn_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_knn_loaded.score(X_train, y_train))\n",
    "print(\"test score\", gs_knn_loaded.score(X_test, y_test))\n",
    "print(\"best params:\", gs_knn_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('dt', DecisionTreeClassifier(random_state = 42))\n",
    "\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'dt__max_depth': [3, 5],  \n",
    "    'dt__min_samples_split': [3, 5, 7],\n",
    "    'dt__min_samples_leaf': [3, 5], \n",
    "#     'dt__random_state': 42\n",
    "#     'knn__n_neighbors' : [3, 5, 10, 15, 20]\n",
    "#             'cvec__stop_words' : [None, 'english'],\n",
    "#            'logreg__penalty' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "gs_dt = GridSearchCV(pipe_dt, params, cv=5)\n",
    "gs_dt.fit(X_train, y_train)\n",
    "\n",
    "with open(\"./pickles/gs_dt_4a.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_dt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9902869561360713\n",
      "test score 0.9895107996629263\n",
      "best params: {'dt__max_depth': 5, 'dt__min_samples_leaf': 3, 'dt__min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_dt_4a.pkl\", 'rb') as f:\n",
    "    gs_dt_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_dt_loaded.score(X_train, y_train))\n",
    "print(\"test score\", gs_dt_loaded.score(X_test, y_test))\n",
    "print(\"best params:\", gs_dt_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer())\n",
    "            ]))\n",
    "    ])),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'rf__n_estimators': [100,125],\n",
    "    'rf__max_depth': [None, 4, 5, 6],\n",
    "    'rf__max_features': [None,\"auto\"]}\n",
    "    \n",
    "\n",
    "\n",
    "gs_rf = GridSearchCV(pipe_rf, params, cv=5)\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "with open(\"./pickles/gs_rf_4a.pkl\", 'wb') as f:\n",
    "    pickle.dump(gs_rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load grid-search object so do not have to re-run the model if accessing notebook later\n",
    "with open(\"./pickles/gs_rf_4a.pkl\", 'rb') as f:\n",
    "    gs_rf_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_rf_loaded.score(X_train, y_train))\n",
    "print(\"test score\", gs_rf_loaded.score(X_test, y_test))\n",
    "print(\"best params:\", gs_rf_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Function that Returns a Confusion Matrix as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion(y_test, preds, classes):\n",
    "\n",
    "    conmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy Score: {accuracy_score(y_test, preds)}')\n",
    "    print(f'Precision Score: {precision_score(y_test, preds)}')\n",
    "    print(f'Recall Score: {recall_score(y_test, preds)}')\n",
    "    return pd.DataFrame(conmat, columns=['Predicted ' +class_ for class_ in classes], \\\n",
    "                index=['Actual '+ class_ for class_ in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling `make_confusion` function to get Accuracy, Precision and Recalls Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a function to print out a nice confusion matrix\n",
    "preds = gs_rf_loaded.best_estimator_.predict(X_test)\n",
    "\n",
    "make_confusion(y_test, preds, [\"wasn't answered\", \"was answered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients - Words Most Indicitive if the Question Will Get Answered or Not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Logistic Regression For Extracting Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load logistic regression grid-search object\n",
    "with open(\"./pickles/gs_logreg_4a.pkl\", 'rb') as f:\n",
    "    gs_logreg_loaded = pickle.load(f)\n",
    "\n",
    "print(\"train score\", gs_logreg_loaded.score(X_train, y_train))\n",
    "print(\"test score\", gs_logreg_loaded.score(X_test, y_test))\n",
    "print(\"best params:\", gs_logreg_loaded.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features and Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg_loaded.best_estimator_.named_steps['features'].transformer_list[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gs_logreg_loaded.best_estimator_.named_steps['features'].transformer_list[1][1].named_steps['cvec'].get_feature_names_out()\n",
    "top_features = np.asarray(['word_count','sentiment_score'])\n",
    "features = np.append(top_features, words)\n",
    "\n",
    "coefficients = gs_logreg_loaded.best_estimator_.named_steps['logreg'].coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A Data Frame with Coefficients and Exponentiated Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'features': features, \n",
    "              'coef' : coefficients,\n",
    "              'exp_coef': [np.exp(coef) for coef in coefficients] #exponentiated coefficients\n",
    "             })\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Most Indicitive of a Question Being Answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = coef_df.set_index('features')\n",
    "coef_df = coef_df.sort_values('exp_coef', ascending = False)\n",
    "coef_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Most Indicitive of a Question NOT Being Answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Top Words for Questions Answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = coef_df['exp_coef'].head(10).sort_values()\n",
    "labels = weights.index\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.barh(labels, weights, color = 'orange')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel('Exponential Coefficient', fontsize=30)\n",
    "plt.title(f'Top 10 Features Predicting that Question Will Be Answered ', fontsize=42)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without word_count:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = coef_df['exp_coef'].sort_values(ascending = False)[1:11].sort_values()\n",
    "labels = weights.index\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.barh(labels, weights, color = 'orange')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel('Exponential Coefficient', fontsize=30)\n",
    "plt.title(f'Top 10 Features Predicting that Question Will Be Answered\\n excluding word_count ', fontsize=42)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Top Words for Questions NOT Answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = coef_df['exp_coef'].tail(10).sort_values()\n",
    "labels = weights.index\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.barh(labels, weights, color = 'skyblue')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel('Exponential Coefficient', fontsize=30)\n",
    "plt.title(f\"Top 10 Features Predicting it Won't be Answered\", fontsize=42)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "280.998px",
    "width": "389.444px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 266.055222,
   "position": {
    "height": "40px",
    "left": "1140.1px",
    "right": "20px",
    "top": "118px",
    "width": "408.875px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
